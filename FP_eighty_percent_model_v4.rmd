---
title: "Final Project"
output: word_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE)
```
```{r}
#Use to display 
library(readr)
# Use Convert POSTX time stamps
library(anytime)
library(dplyr)
library(data.table)
library(topicmodels) #topic modeling functions
library(stringr) #common string functions
library(tidytext) #tidy text analysis
library(tidyverse) #data manipulation and visualization
library(scales) #used for percent scale on confusion table
library(ggthemes)
library(ggplot2)
library(anytime)
library(GGally)
library(car)
library(ggcorrplot)
library(dplyr)
library(fastDummies)
library(corrplot)
library(ggpubr)
library(ggthemr)
ggthemr("flat")
set_swatch(c(swatch(),'purple','black'))
```
Load the raw data
```{r, include=FALSE}
#Load the origonal TED Talk data provided
# Use readr package to display data types of variables
TED.df<- read_csv("ted_main.csv")
```

#Description of Research Question:
The goal of this analysis asses varibles that measure the popularity of a TED Talksand discover which assocaited characteristicso predict this measure. Further more we are intrested in comparing how predictors change with respect to a TED Talks genre and time of publication.

#Description of scientific or research issues:
The initial data contained a large amount of variables, which made it difficult to distinct the significant variables from non-significant ones. Furthermore, after analyzing the nature of the data is not normal, so it limited our ability to implment statical inference. Additonally, the nature in which the varibes was
recoreded is ambigous which poses a risk of bias in their measurments.

Certian varibles provided of limited predicting power as they are only recored once video has been published.
It was our view that in predicitng the popularity of a video before hand we would want to consider its
state of characteristics before release.

Furthermore, the subjective defintion of Popularity poses ambguity in definitive measure. As investigators
we have our own opinons on what consitutes popularity however, different auidences my vary in their opnion
to this.

#Description of Data:

In order to asses trends of time frames we needed to convert the provided 
publish dates into a numerically readable format that would allow observations to be
classifed by day month and year. We opted not asses the film date as we belived the publication
date was more reflective of audiences opions at that time. Below we create 3 varibles of Year,Month, and day
in which to asses trends over time throughout our analysis.

```{r,message=FALSE}
library(lubridate)
# Convert time stamps to readable format
TED.df$published_date <- anydate(TED.df$published_date)
TED.df$year <- year(TED.df$published_date)
TED.df$month <- month(TED.df$published_date)
#TED.df$month =  as.Date(paste0(TED.df$year,"-",TED.df$month,"-01"),"%Y-%m-%d")
#TED.df$year = as.Date(as.character(TED.df$year), format = "%Y")

TED.df$day = weekdays(TED.df$published_date,abbreviate = TRUE)
TED.df$DaysAvailable <-difftime("2017-09-23", TED.df$published_date,units = c("days"))
TED.df$DaysAvailable <- as.numeric(round(TED.df$DaysAvailable))

```

A characteristic we hypothisied could predict popularity was is the view counted of videos related to it.
The related talks appear at the bottom of the web page, so, if the related talks are popular(measured by the view count), there is a high chance that after watching them people would also watch other talks related to them. In order to find that out, we decided to take a views count of related talks for each individual Ted Talk, and take an average of that value.
```{r}
#TED.df["id"]=NA
#num=1

#TED.df$id=seq(num, 2550, 1)
#ted_pop_views=arrange(TED.df, desc(views))
total=data.frame(matrix(ncol = 3, nrow =2550))
colnames(total)<-c("AvgRelViews", "TotalRelViews","TotalRelated")
r<-1
for (i in TED.df$related_talks){
  
  tit=str_match_all(i,  "(viewed_count':\\s)(\\d*)")[[1]][,3]
  count=0
  total[r,2]=TED.df$views[r]
  x<-0
    for (k in tit){
      find_views=regexpr("\\d+",k)
      x<-x+as.numeric(regmatches(k,find_views))
      count=count+1
    }
  total[r,2]= x
  total[r,3]= count
  x<-x/count
  
  total[r,1]=x
  r=r+1
}
TED.df$AvgRelViews <- total$AvgRelViews
TED.df$TotalRelViews <- total$TotalRelViews
TED.df$RelatedCount <- total$TotalRelated
```



Create Sentitment Scores and Counts:
In order to easily evaluate how ratings influence the popularity, we decided to create the sentiment score system, by extracting the number of times, the video has received certain evaluation(i.e "Persuasive", "Informative", e.c.t), determinig whether it was negative or positive, and then counting evaluation per number of views. Furthermore we recored the indvidual counrts of rating to explore if a spefic feeling toward a vidoe better predicted it popularity.
```{r,warning=FALSE}
rating_to_list <- function(vec){
x <- str_extract_all(vec, "\\w+")
x <- unlist(str_extract_all(x, "\\w+"))
return (x)
}


find_rating_count <- function(vec, rat_tag){
  count <- unlist(vec)[which(unlist(vec) == rat_tag) +2]
  return (as.numeric(count))
}
ratingTypes <- c("Persuasive", "Unconvincing", "Longwinded", "Informative", "Beautiful", "Confusing", "Obnoxious", "dropping", "Ingenious", "OK", "Courageous", "Inspiring", "Funny", "Fascinating")
rateList <-lapply(TED.df$ratings, rating_to_list)

for (i in 1:length(ratingTypes)){
  TED.df[ratingTypes[i]] <- sapply(rateList, find_rating_count, rat_tag = ratingTypes[i])
TED.df[ratingTypes[i]] <- replace(TED.df[ratingTypes[i]],is.na(TED.df[ratingTypes[i]]),0)
TED.df[ratingTypes[i]] <- lapply(TED.df[ratingTypes[i]],as.numeric)
}
colnames(TED.df)[colnames(TED.df) == 'dropping'] <- 'jawdropping'

sentPOS <- c(1,0,0,1,1,0,0,1,1,0,1,1,1,1)
sentNEG <- c(0,1,1,0,0,1,1,0,0,0,0,0,0,0)
TED.df["sentPOS"] <- as.matrix(TED.df[,25:38]) %*% sentPOS
TED.df["sentNEG"] <- as.matrix(TED.df[,25:38]) %*% sentNEG
TED.df["netSENT"]<- TED.df$sentPOS - TED.df$sentNEG
TED.df["CR"] <- (TED.df$comments / TED.df$views)

TED.df["sentSUM"] <- TED.df$sentPOS + TED.df$sentNEG
TED.df["SR"] <- TED.df$sentSUM / TED.df$views
TED.df["NEGRatio"]<- TED.df$sentNEG /TED.df$sentSUM

TED.df$titleLen <- lapply(TED.df$title,FUN = nchar)
TED.df$descriptionLen <- lapply(TED.df$description,FUN = nchar)

TED.df$titleLen <- as.numeric(TED.df$titleLen)
TED.df$descriptionLen <- as.numeric(TED.df$descriptionLen)

TED.df$activity <- TED.df$CR / TED.df$SR
#TED.df["activity"] <- TED.df$sentSUM/TED.df$comments
#TED.df["sentQUO"]<- TED.df$sentPOS / TED.df$sentNEG

```

Within the varibles of Tags, Title and description were a large amount of text. We belived that the subject
nature of a video could be determined by assesing these in combination. In order employ a varible which
could be used in inference of popularity we attempted to group videos into topic groups based ont combination of this text data. 

THis process was accomplished through a Latent Dirchlet Analysis Topic Model (REFENCE)
In order to conduct this anlaysis we need to prepare the data for the LDE model. Remove all the unnesseccary tags, such as "TED Fellows", "TEDEx", "TED Brain Trust", "TED MEd" and "talks", since they do not have a significant role for grouping topics. Additonally we removed stop words such as and if or , etc. which did not provide interatability toward a specific topic. Finally we removed words with less than 5 occcurances amongst all videos text. We then created the document term matrix with the frequency of each unqie word for each video. THis was then inplmented into the LDA Topic Model which will be discussed later in analysis.

Additonally while removing unusfle text we were able to create a new varible of event type between regulard TED Talks and TEDx. We opted touse this varible in place the event varible provied is it was not as diluted by fragmented categories of  city event loactions of the orginal event varible.We thought, since the TED and TEDx is intended for different kind of public(TED covers more global issues and focuses on the global community, while TEDx is typically intended for local community and voices local issues), the distribution of popuar videos among these two groups might differ as well.
```{r}

library(quanteda)
library(topicmodels)
#Create Document Term Matrix
k <-2
text <- vector(,length = length(TED.df$tags))
eventType <- vector(,length = length(TED.df$tags))

for (i in 1:length(TED.df$tags)){
  description <- TED.df[i,2]
  tag <- TED.df[i,14]
  title <- TED.df[i,15]
  if (str_detect(tag,"TED Fellows")){
    tag <- str_remove_all(tag,"TED Fellows")
  }
  else if (str_detect(tag,"TEDx")){
     eventType[i] <-"TEDx"
    tag <- str_remove_all(tag,"TEDx")
  }
  else if (str_detect(tag,"TED Brain Trust")){
    tag <- str_remove_all(tag,"TED Brain Trust")
  }
  else if (str_detect(tag,"TEDMED")){
    tag <- str_remove_all(tag,"TEDMED")
  }
  tag <- str_remove_all(tag,"TED")
  description <- str_remove_all(description,"talk")
  description <- str_remove_all(description,"can")
  text[i] <- paste(description,title,tag,sep = " ")
}
eventType[eventType == FALSE] <- "TED"
TED.df$text <- text
TED.df$eventType <- eventType

DTM <- dfm(TED.df$text,remove_punct = TRUE,remove = stopwords("en") )
doc_freq <- docfreq(DTM)
DTM <- DTM[,doc_freq>=5]
#DTM <- dfm_tfidf(DTM)
par_DTM <- convert(DTM, to = "topicmodels")
```



In prediciting the popularity of TED talks there are a varity of ways in which
popularity can be defined. Total views acts as an estimate of the size
of the audience the video has been able to reach. However, there is no
detailed information on how long viewers watched the video for or whether they
left soon after starting a video. To combat this ambiguity we opted to include a measure of user activity in our classification of popularity  

We defined a new measure called the comment ratio which is the total number
of comments divided by the total views of a video. The intution behind this statistic was to judge user activity realtive to the amount of views. 

It was then decided to classify a video as popular if had both above median views and an above median comment ratio. Our logic here was if it measured 
high on both these stats it meant many individuals were viewing the video and engaging in discussion about it.

We considered including the sentiment score caluation into this measure however we found that it did not offer clear differination between videos. we wanted the oppourunity
to asses individual rating types effect on popularity without having a bias towards the measure. 

Implementing this classification rule lead to the following segementation of the data. As we can see from the scatterplot, there is a high correlation between views and comments, so we are going to take the comment ratio as one of our main variable to classify whether the video is popular or not. THerefore, the variable isPop was defined

```{r}
options(scipen = 999)
medComments <- median(TED.df$comments)
medView<- median(TED.df$views)
medCR<- median(TED.df$CR)
medComments <- median(TED.df$comments)
medNR <- median(TED.df$NEGRatio)
TED.df <- mutate(TED.df,isPop = ifelse((views > medView) & (CR> medCR),"1","0"))

PopVar<- c("views","comments","CR","isPop")
PopVar.df<- select(TED.df,PopVar)

gg <- ggplot(TED.df,aes(x=views,y=comments)) + geom_point(aes(col=isPop,size=CR)) + labs(subtitle="Views vs Comments",x="Views",y="comments",title="Scatterplot") +  theme_stata() + scale_fill_brewer(palette = "Set1")
plot(gg)

```


Tis revaled the following structure of Popular vs non-counts. We see combing the elemtns of view counts and relative user activty by comment ratios it distingushes a more elite class of 500 vidoes who charatiscs we wish to explore.
```{r,include=F}

percentData <- TED.df %>% count(isPop) %>% mutate(ratio=percent(n/sum(n)))
ggplot(TED.df,aes(x = as.factor(isPop),fill = as.factor(isPop))) + geom_bar( position = "dodge",stat = "count",color="black") + labs(x=NULL,fill = "Is Popular",title = "Popular Video Counts")  + geom_text(stat = 'count',aes(label=..count..), vjust=-.5) + geom_text(data=percentData, aes(y=n,label=ratio),
              position=position_dodge(width=0.9), vjust=3)
```
Our next goal was to determine the features that lead to these classifications
We first attempted a factor analysis on the quantative varibles to explore possibilites of varible reductions. However as seen below this  was unsucessful in revaling a simple
structures to the data with two factors explaing a small portion of the variance in both Barllett's and the Regression methods calculation of factors.
```{r}
##### ------------------------------FACTOR ANALYSIS ----------------------------
keeps <- c("isPop","AvgRelViews","RelatedCount","duration","languages","num_speaker","DaysAvailable","titleLen","descriptionLen","CR","Persuasive","Unconvincing","Longwinded","Informative","Beautiful","Confusing","Obnoxious","jawdropping","Ingenious" ,"OK","Courageous","Inspiring","Funny","Fascinating")
general <- select(TED.df, keeps)
general$isPop <- as.numeric(general$isPop)



fac.ted1 <- factanal(general, 2, rotation="promax", scores="Bartlett")
#print(loadings(fac.ted))
#plot(fac.ted$scores, pch=20)
#par(mfrow=c(1,2))
#qqnorm(fac.ted$scores[,1])
#qqnorm(fac.ted$scores[,2])

fac.ted2 <- factanal(general, 2, rotation="promax", scores="regression")
print(loadings(fac.ted1),loadings(fac.ted2))
#plot(fac.ted$scores, pch=20)
#par(mfrow=c(1,2))
#qqnorm(fac.ted$scores[,1])
#qqnorm(fac.ted$scores[,2])

#promax rotation produces simple structure
#number of speakers has been annhilated in all factors - remove from analysis
#small covariance/correlation to the other dependent variables

```
Our next approach was to asses how popularity differed by different categorical varibles. We first assed how the counts of popular videos differed by eventTypes previously defined above. We dont see a sizable differnce between the propotions of poular videos amongst the categories. Perhaps  further segmentation later in the analysis will reval a more interesting structre amongst these types.
```{r}
percentData <- TED.df %>% group_by(eventType) %>% count(isPop) %>% mutate(ratio=percent(n/sum(n)))

ggplot(TED.df,aes(x = as.factor(isPop),fill = eventType)) + geom_bar( position = "dodge",stat = "count",color="black") + labs(x=NULL,title ="Popular Video Counts",subtitle = "By Event Type") +    geom_text(stat = 'count',aes(label=..count..),position=position_dodge(width=0.95), vjust=-0.25) + scale_x_discrete(breaks = c("0","1"),labels = c("Not Popular","Popular")) + geom_text(data=percentData, aes(y=n,label=ratio),
              position=position_dodge(width=.95), vjust=1.25,size=3)
```
We compared the means of views,comment ratios, count of Ratings and counts of popular talks  between the event types using mananova. We see using the Wilk's lambda we reject that Cov variance matrixes are equal. Thus inference between means is limited.
```{r}
library(mvnormtest)
data <- select(TED.df,views,CR,sentSUM,isPop,eventType)
data$isPop <- as.numeric(data$isPop)
data <- select(TED.df,views,CR,sentSUM,isPop,eventType)
data$isPop <- as.numeric(data$isPop)
res.man <- manova(as.matrix(data[,1:4]) ~ eventType,data = data)
summary(res.man,test="Wilks")
summary(res.man)

summary.aov(res.man)
```

We further visualized the difference of rating values between populatiry by ploting the respective counts of rating types between not Popular and Popular segments. We first hilight that the popular videos have larger
counts in all ratingtypes. This provides evidence hat having larger views and and comments activty will coresspond with higher amounts of ratigs. Amongst the postive rating types inspiring has signifcantly greater count in both popularity groups. Additonalley amongst the negative counts unispiring shares this proprtey of the max count amongst popularity groups. This [rovides evidence towards a corwding effect of perceptions by viewers.Possibly if there is all ready a majority of people tending towards a rating type, they wil also tend toward this sentiment.
```{r}
library(reshape2)
TED.df$isPop <- as.numeric(TED.df$isPop)
basic <- c("isPop","AvgRelViews","TotalRelViews","RelatedCount","duration","languages","num_speaker","DaysAvailable","titleLen","descriptionLen")
Pop.basic <- select(TED.df,basic)
sentiment <- c("isPop","Persuasive","Unconvincing","Longwinded","Informative","Beautiful","Confusing","Obnoxious","jawdropping","Ingenious" ,"OK","Courageous","Inspiring","Funny","Fascinating")
Pop.sentiment <-select(TED.df,sentiment)
fun_mean <- function(x){
  return(data.frame(y=mean(x),label=round(mean(x,na.rm=T))))}
Pop.basic$isPop <- as.character(Pop.basic$isPop)
Pop.sentiment$isPop <- as.character(Pop.sentiment$isPop)
m <- melt(Pop.sentiment,id.vars='isPop',measure.vars=c("Persuasive","Informative","Beautiful","jawdropping","Ingenious" ,"Courageous","Inspiring","Funny","Fascinating","OK"))

g <- ggplot(m,aes(x=isPop, y=value, fill=variable))
g + geom_bar(stat = "summary", fun.y = "mean",position = position_dodge(width = 0.9)) + geom_pointrange(stat = "summary", fun.data = "mean_se",position = position_dodge(width = 0.9)) + labs(x=NULL,title = "Counts of Ratios",subtitle = "Positive Sentiment") + scale_x_discrete(breaks = c("0","1"),labels = c("Not Popular","Popular"))

 
m <- melt(Pop.sentiment,id.vars='isPop',measure.vars=c("Unconvincing","Longwinded","Confusing","Obnoxious")) 

g <- ggplot(m,aes(x=isPop, y=value, fill=variable))
g + geom_bar(stat = "summary", fun.y = "mean",position = position_dodge(width = 0.9)) + geom_pointrange(stat = "summary", fun.data = "mean_se",position = position_dodge(width = 0.9)) + labs(x=NULL,title = "Counts of Ratios",subtitle = "Negative Sentiment") + scale_x_discrete(breaks = c("0","1"),labels = c("Not Popular","Popular"))

#g + geom_boxplot(varwidth = F ) + labs(title = "Boxplot",subtitle = "Basic Varibles") + theme(axis.text.x = element_text(angle=65, vjust=0.6))  + stat_summary(fun.y = mean, geom="point",colour="darkred", size=3) +stat_summary(fun.data = fun_mean, geom="text", vjust=-0.7) +  scale_x_discrete(breaks = c("0","1"),labels = c("Not Popular","Popular"))

```



In order to better understand the relative effects each rating varibles counts has on its respective video we decided to convert ratings counts to their proprtions amongst the total number of ratings for each respective video.

```{r}
TED.df[,25:38] <- (1/TED.df$sentSUM)*TED.df[,25:38]
```
To get a better sense the relationships between popularity and the feature varibles we asses the correleation matrix for the quanatative varibles.


```{r}
TED.df$isPop <- as.numeric(TED.df$isPop) 
basic <- c("isPop","AvgRelViews","TotalRelViews","RelatedCount","duration","languages","num_speaker","DaysAvailable","titleLen","descriptionLen")
Pop.basic <- select(TED.df,basic)
sentiment <- c("isPop","Persuasive","Unconvincing","Longwinded","Informative","Beautiful","Confusing","Obnoxious","jawdropping","Ingenious" ,"OK","Courageous","Inspiring","Funny","Fascinating")
Pop.sentiment <-select(TED.df,sentiment)

basic.corr <- round(cor(Pop.basic[,1:10]),1)
ggcorrplot(basic.corr, hc.order = FALSE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title="Correlogram of Basic Feature Variables", 
           ggtheme=theme_bw)
sentiment.corr <-  round(cor(Pop.sentiment[,1:15]),1)
ggcorrplot(sentiment.corr, hc.order = FALSE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title="Correlogram of Sentiment Variables", 
           ggtheme=theme_bw)
```
This raises some intresting findings. Languages having the highest correlation with popularity and  could indicate allowing the video to be understood by a more gloabl audience is resulting in larger groups viewing and interactig on videos.There is also a variety of minor postive and negative correlations amongst the ratings ratios. Before jumping to any conclusions it important that we consider how these effects change based on segements of categorical varibles in additon to time.




On the graph Average number views that are distributed all over years 2007 to 2018 we can see that the highest number of views occured in 2007, following by a huge decline and a small spike in 2013, again following the downwards trend to the 2018. This can be backed up by the fact that Ted platform was launched in 2007( that is why we can observe such a huge number of views), and in March 2012 Netflix announced a deal to stream series of Ted Talks(that explaines a small upward trend in the number of views, since expansion of Ted Talks to more platfoms provides access for bigger amount of population).

The graph average comments distribution per year indicates that there were major spikes of upward trend in 2011 and in 2014. Althought, the nature of the trends is not clear, we can theorize that the spike in comments could be influenced by some amount of talks that were appealing for majority of people during that time(could be caused by political or media influence). This theory could be backed up by the next graph that indicates that the most popular videos also appeared on 2011 and 2014, which proves that our choice of including the comment per view variable into our model that defines the popularity.

Average number of languages by years for both popular and unpopular videos show an overall decreasing trend. It can be inferred that, with increasing access to technology and English becoming a far more popular language in foreign countries, the need for the TED talks to be translated into different languages has declined. English has had great success as the lingua franca of business, travel, and international relations. 

Number of views by month shows that there are less views, averaging around 300000000, in the winter months. There is an increase in views in March and a gradual decline into the summer months, ending in August. It is difficult to infer a particular trend in views by month because the audience varies so greatly. Furthermore, the count of popular and unpopular videos published by month does not vary greatly. Therefore, it cannot be inferred that number of views in any given month increases or decreases with the number of videos that were published in that month.

In the graph showing numbers of views by month by year, there is an upward trend of views from 2006 to 2013 and then a downward trend afterwards. In general, it seems that the month with the greatest number of views is May. This may be attributed to university students ending their winter semester in April and finding further educational value from these talks. However, as previously stated, the audience varies so greatly that it is difficult to make any specific inference about these trends without further information. Finally, the downward trend post 2013 may show that TED has declined in popularity and people are less likely to watch their videos.

The count of popular videos by month and by year shows an interesting trend. It is clear that post 2013, the number of "not popular" videos has greatly declined, to the point where they zeem virtually nonexistent in 2016 and 2017.


```{r,fig.width=10}
#TED.df$month =  as.Date(paste0(TED.df$year,"-",TED.df$month,"-01"),"%Y-%m-%d")
#TED.df$year = as.Date(as.character(TED.df$year), format = "%Y")
#Year Analysis:
# Y is numeric
ggplot(TED.df, aes(x = as.Date(as.character(year), format = "%Y"), y = languages)) + stat_summary(fun.y = mean, geom = "line",size=3) + labs(title = "Average Number of Languages",subtitle = "By Year",x="Year",y="Count") + scale_x_date(labels = date_format("%Y")) + facet_wrap(~mutate(TED.df,isPop=if_else(isPop == "0","Not Popular","Popular"))$isPop)

ggplot(TED.df, aes(x = as.Date(as.character(year), format = "%Y"), y = views)) + stat_summary(fun.y = mean, geom = "line",size=3) + labs(title = "Average Number of Views",subtitle = "By Year",x="Year",y="Count") + scale_x_date(labels = date_format("%Y"))


ggplot(TED.df, aes(x = as.Date(as.character(year), format = "%Y"), y = comments)) + stat_summary(fun.y = mean, geom = "line",size=3) + labs(title = "Average Number of comments",subtitle = "By Year",x="Year",y="Count") + scale_x_date(labels = date_format("%Y")) 


# Y is categorical
ggplot(TED.df, aes(x = as.Date(as.character(year), format = "%Y"), color = as.factor(isPop))) + geom_line(stat='count',size=3) + labs(title = "Counts of Popular Videos",subtitle = "By Year",x="Year",y="Count",color="Is Popular") + scale_x_date(labels = date_format("%Y")) 

ggplot(TED.df, aes(x = as.Date(as.character(year), format = "%Y"), color = as.factor(isPop))) + geom_line(stat='count',size=3) + labs(title = "Counts of Popular Videos",subtitle = "By Year and Event Type",x="Year",y="Count",color="Is Popular") + scale_x_date(labels = date_format("%Y")) + facet_wrap(~eventType)

#Month Analysis:
#Y is Numeric
ggplot(TED.df,aes(x=as.Date(paste0("2015-",TED.df$month,"-01"),"%Y-%m-%d"),y=views)) +  stat_summary(fun.y = sum, geom = "bar")+labs(x="Month") + labs(title = "" ,subtitle ="By Month" ) + scale_x_date(date_labels = "%b")

#Y is Categorical
ggplot(TED.df, aes(x = as.Date(paste0("2015-",TED.df$month,"-01"),"%Y-%m-%d"), fill = as.factor(isPop))) + geom_bar(stat='count',size=3,position = "dodge") + labs(title = "Counts of Popular Videos",subtitle = "By Month",x="Months",y="Count",fill="Is Popular") + scale_x_date(date_labels = "%b")

#Month & Year Analysis
ggplot(TED.df,aes(x=as.Date(paste0("2015-",TED.df$month,"-01"),"%Y-%m-%d"),y=views)) +  stat_summary(fun.y = sum, geom = "bar")+labs(x="Month") + scale_x_date(date_labels = "%b") + facet_wrap(~year)

ggplot(TED.df, aes(x = as.Date(paste0("1999-",TED.df$month,"-01"),"%Y-%m-%d"), fill = as.factor(isPop))) + geom_bar(stat='count',position = position_dodge2(width = NULL, preserve = c("total", "single"),
  padding = 0.1, reverse = FALSE),size=2) + labs(title = "Counts of Popular Videos",subtitle = "By Month and Year",x="Months",y="Count",fill="Is Popular") + scale_x_date(date_labels = "%b") + facet_wrap(~year)
```




We now turn our attention to assesing characteristic of  topic types of the diffrent videos. 
Using the

Using the data term matrix previously created we  evaluated the preforamnce of grouping videos into differnt k number of topics.
TO brefiley summarize the LDA; it is a generative probablistic model in which objects are broken into smaller pieces  that comprise them. A singluar piece for specfic object may be identical unique amongst other objects in the model.The models probablistic setup is to group the pieces amongst all objects in into k number of topic groups. With this the probability of sampeling a sepcific piece from a topic comprises one distribution. Addiotonally based on the occourences of pieces in an object the probability of sampelling a topic type from a object can be derived. 

In our situation the vidoes represent the objects and the words contained in their title,description and tags are that peices they embody.The model aims to estimates two varibles; the probability of a word belinging to a topic denoted beta and the probability of a group belonging to a topic denoted gamma. Provided a training set of data it learns these paramter by using method of Gibbs sampeling. The algorithum goes through each videos text data and randomly assigns eacg word within the text data to one of the k topic groups. This forms intial probabilty disttributions for words in topics and topics in videos. It then goes through each word in each document and evaluates two conditional probailities. The proportion of the words in the document have been assigned to an indival topic t ie) P(t|v) and the proportion of that topic t over all vidoes that come from that specfic word ie) P(w|t). It then reassigns that word to one of the topic groups with probaility P(t|v) x P(w|t) ie) the probability that topic t generate that word. After repeating this process a large number of times it eventually reaches a steady state for both probability distributions in which then the process and ends and these are estimates used.

When then can classify each video by comparing the the probability of the topic given the video for each topic ie) gamma  and assigning the topic to highest probability.

Before the model can be learned the hyper parameter of k number of groups must be decided.In making this decsion evaluated below model results for differnt levels. Models were comapred based on their individual loglikelihoods given the training data and the perplexuity of each model. Perplexity like likelihood is a measure of how well the model predicted the data.It is however a measure of uncertianty in a model.  A random varible with perplexity k has the same uncertianty as a k-sided dice.

```{r,eval=FALSE}
n_topics <- c(2, 4, 10, 20, 24, 26,30,32,34,36,38,40,50)
lda_compare <- n_topics %>%
  map(LDA, x = par_DTM, control = list(seed = 747))
data_frame(k = n_topics,
           perplex = map_dbl(lda_compare, perplexity)) %>%
  ggplot(aes(k, perplex)) +
  geom_point() +
  geom_line() +
  labs(title = "Evaluating LDA topic models",
       subtitle = "Optimal number of topics (smaller is better)",
       x = "Number of topics",
       y = "Perplexity")
data_frame(k = n_topics,
           perplex = map_dbl(lda_compare, logLik)) %>%
    ggplot(aes(k, perplex)) +
    geom_point() +
    geom_line() +
    labs(title = "Evaluating LDA topic models",
         subtitle = "Optimal number of topics (larger is better)",
         x = "Number of topics",
         y = "score")


```

Its important to hilight that with this type of unsupervised learing it recommend to not soley base the number of topics off perplexuity and log-likelihood but also to asses where the topic types have an intutive sense to them. ie) whether words are being segemented in to topics have an intuitive interpertation. With this in mind and incomapring the results we saw that large number of topic groups were measuring worse in both loglikelihood and perplexity.So we wanted to limit our choice of k to smaller numbers. We saw tha when three was chosen the clearest distinction of topics dispalyed itself between the groups and continued our anlysis with LDA model derived from 3 groups.

Seen below are the words with the highest probability of belonging to the respective topic group. The model classifies the video into one of these topic
groups based on which topic corresponds to the highest probability assocaited with its respective words and frequncies.

In certian cases video have equal probabilites amongst each topic group. In this secnario we decied to classify it an arbitrary general topic group
which does not have a focoused them. 

We see that topic 1 hase them of modernism/technology, topic 2 a theme of public and international affairs and 3 a theme of culture/art .
Observation that scored equal probabilites for each topic were assigned a topic value of 0 reprsenting the general topic group. 



```{r}
#Select model with best k

lda_model <-LDA(par_DTM, method = "Gibbs",k = 3,control =list(seed=747))

terms <- tidy(lda_model)
topTerms <- terms %>% group_by(topic) %>% top_n(5,beta) %>% ungroup() %>% arrange(topic,-beta)

topTerms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 2) +
  coord_flip()

#terms(lda_model,5)
obs_gamma <- tidy(lda_model, matrix="gamma")
obs_gamma$document <- rep(TED.df$title,1)
obs_gamma$id <-seq(1, 2550, 1)
topic <- obs_gamma %>% group_by(document) %>% top_n(1,gamma)
topic$dup <- duplicated(topic[,4])
topic[duplicated(topic$id)|duplicated(topic$id, fromLast=TRUE),2] = "0"
topic <- topic %>% group_by(document) %>% sample_n(1) %>% ungroup()
topic$topic <- as.character(topic$topic)
topic <-arrange(topic,document)
#topic <- dummy_cols(topic,select_columns = "topic")
topicNames <- colnames(topic)
TED.df <- arrange(TED.df,title)
TED.df$topic <- topic$topic
```
We now evaluate how popularity differs amongst the respective groups. We see that amoung the popular videos Topic 2(public and international affaris) has the most view count. It makes sense, since art/culture and modernism/technology are focused more on groups of people who either work in these sectors or have a personal preference towards them ,while public and global issues are intended for majority of people, no matter their personal preferences or emloyment.
```{r}
TED.df$isPop <- as.numeric(TED.df$isPop)
basic <- c("isPop","AvgRelViews","TotalRelViews","RelatedCount","duration","languages","num_speaker","DaysAvailable","titleLen","descriptionLen","topic")
Pop.basic <- select(TED.df,basic)
sentiment <- c("isPop","Persuasive","Unconvincing","Longwinded","Informative","Beautiful","Confusing","Obnoxious","jawdropping","Ingenious" ,"OK","Courageous","Inspiring","Funny","Fascinating","topic")
Pop.sentiment <-select(TED.df,sentiment)
percentData <- TED.df %>% group_by(topic) %>% count(isPop) %>% mutate(ratio=percent(n/sum(n))) %>% ungroup()

ggplot(TED.df,aes(x = as.factor(isPop),fill = topic)) + geom_bar( position = "dodge",stat = "count",color="black") + labs(x=NULL,title ="Popular Video Counts",subtitle = "By Topic Groups") +    geom_text(stat = 'count',aes(label=..count..),position=position_dodge(width=0.9), vjust=-0.25) + scale_x_discrete(breaks = c("0","1"),labels = c("Not Popular","Popular")) + geom_text(data=percentData, aes(y=n,label=ratio),
              position=position_dodge(width=0.9), vjust=3)
```
In attmeping MANOVA we  see that an assumption of equal covaraince is rejected by wilks lambda, possibly due to the fact theat the nature of data is not normal.
```{r}
data <- select(TED.df,views,CR,sentSUM,isPop,topic)
data$isPop <- as.numeric(data$isPop)

res.man <- manova(as.matrix(data[,1:4]) ~ topic,data = data)
summary(res.man,test="Wilks")

```

Further more when comparing just the topic groups 1-3 Equal covarince was still rejected by Wilk's Lambda. Again, since the nature of data is not normal, it makes sensethat the test by Wilk's Lambda fails.
```{r}
data <- filter(data,topic != 0)
res.man <- manova(as.matrix(data[,1:4]) ~ topic,data = data)
summary(res.man,test="Wilks")
```


In assesing the the popularity of events types amongst the topic groups we see that group 2 has both most popular TED and TEDx videos. We thought, since the TED and TEDx are intended for different kind of public(TED covers more global issues and focuses on the global community, while TEDx is typically intended for local community and voices local issues), the distribution of popuar videos among these two groups might differ as well. However, there does not appear to be and trend reversals when segmenting by event type. Possibly, again, due to the fact that global/public issues are intended to interested all kinds of people.
```{r}

ggplot(TED.df,aes(x = as.factor(isPop),fill = topic)) + geom_bar( position = "dodge",stat = "count",color="black") + labs(x=NULL,title ="Popular Video Counts",subtitle = "By Topic Groups and Event Types" ) +    geom_text(stat = 'count',aes(label=..count..),position=position_dodge(width=0.9),vjust=-.1 ) + scale_x_discrete(breaks = c("0","1"),labels = c("Not Popular","Popular")) + facet_wrap(~eventType)
```


We now further evaluate relationship amongst popularity with feature varibles by segements of topic groups. There is  any clear major changes in correlations by topic groups. Languages remains the strogest correlation across topics and there are minor differences in correlations amongst the ratings ratios. That makes sense, since if the video is accessible for a bigger population, it will have the higher view count.
We can also observe that while analyzing the sentiment feature variables the correlation between comments that express the same degree of evaluation(i.e "unconvincing" and "confusing" or "inspiring" and "courageous") is also quite high. Again, we can theorize that the crowd effect is taking in, since people are more likely to have the same opinion as the majority of the public.
```{r}

Pop.basic$isPop <- as.numeric(Pop.basic$isPop)
Pop.sentiment$isPop <- as.numeric(Pop.sentiment$isPop)
for(i in unique(Pop.basic$topic)) {
  topic.df <- subset(Pop.basic, topic==i)
  corr<- round(cor(topic.df[,1:6]),1)
  gg <- ggcorrplot(corr, hc.order = FALSE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title= paste("Topic Group",i,"Correlogram of Basic Feature Variables"),
           ggtheme=theme_bw)
  plot(gg)
}

for(i in unique(Pop.basic$topic)) {
  topic.df <- subset(Pop.sentiment, topic==i)
  corr<- round(cor(topic.df[,1:15]),1)
  gg <- ggcorrplot(corr, hc.order = FALSE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title= paste("Topic Group",i,"Correlogram of Sentiment Feature Variables"),
           ggtheme=theme_bw)
  plot(gg)
           }

```

We decided to ananlyze how view count of different topic groups change over time. As we can see, view counts for all topic groups are following roughly the same pattern as the popular videos in general. However, it is important to note, that for the topic group 0(other) and topic group 3(art/culture) there were two significant spikes in 2013 and 2014 respectively. Again, we are uncertain of the nature of such behaviour, but we can theorize that during these years there were events that led people for certain preferences, or there were certain videos in these categories that attacted a lot of people. The latter theory could be backed up by the graph counts of popular videos by year and topic group, since there is a spike in popular videos in 2014 for group 3.

If we look at the graph that shows us average number of comments per group topics we can see, that group 2 is generally the most discussible and has spikes in 2008, 2011 and 2014. Group 0, 1,3 roughly follow the same pattern, but is is important to note that the largest amount of comments belongs to group 0 in 2014.

if we look at the graphs that indicate the counts of topic videos by month, we can see that group 3 tend to public more videos from April to June, while group 3 has a spike in videos in September.
On the graph counts of Topic videos by month and year, we can see that the biggest spike for group 3 was in 2007, group 1 had the highest counts in 2011 and 2012, and starting 2014 group 2 took lead. That could be explained that originally TED Talk was intended for proffesinal masses, and later on expanded to cover the majority of public.



```{r}
# Y is numeric
ggplot(TED.df, aes(x = as.Date(as.character(year), format = "%Y"), y = languages, color=topic)) + stat_summary(fun.y = mean, geom = "line",size=3) + labs(title = "Average Number of Languages",subtitle = "By Year",x="Year",y="Count") + scale_x_date(labels = date_format("%Y")) + facet_wrap(~mutate(TED.df,isPop=if_else(isPop == "0","Not Popular","Popular"))$isPop)

ggplot(TED.df, aes(x = as.Date(as.character(year), format = "%Y"), y = views,color=topic)) + stat_summary(fun.y = mean, geom = "line",size=3) + labs(title = "Average Number of Views",subtitle = "By Year",x="Year",y="Count") + scale_x_date(labels = date_format("%Y"))


ggplot(TED.df, aes(x = as.Date(as.character(year), format = "%Y"), y = comments,color=topic)) + stat_summary(fun.y = mean, geom = "line",size=3) + labs(title = "Average Number of comments",subtitle = "By Year",x="Year",y="Count") + scale_x_date(labels = date_format("%Y"))


# Y is categorical
ggplot(TED.df, aes(x = as.Date(as.character(year), format = "%Y"), color = as.factor(isPop))) + geom_line(stat='count',size=3) + labs(title = "Counts of Popular Videos",subtitle = "By Year and Topic Group",x="Year",y="Count",color="Is Popular") + scale_x_date(labels = date_format("%Y")) + facet_wrap(~topic)


#Month Analysis:
#Y is Numeric
ggplot(TED.df,aes(x=as.Date(paste0("2015-",TED.df$month,"-01"),"%Y-%m-%d"),y=views,fill=topic)) +  stat_summary(fun.y = sum, geom = "bar",position = position_dodge2(width = NULL, preserve = c("total", "single"),padding = 0.1, reverse = FALSE),size=2) +labs(x="Month") + labs(title = "" ,subtitle ="By Month" ) + scale_x_date(date_labels = "%b")

#Y is Categorical
ggplot(TED.df, aes(x = as.Date(paste0("2015-",TED.df$month,"-01"),"%Y-%m-%d"), fill = topic)) + geom_bar(stat='count',size=3,position = "dodge") + labs(title = "Counts of Topic Videos",subtitle = "By Month",x="Months",y="Count") + scale_x_date(date_labels = "%b")

#Month & Year Analysis

ggplot(TED.df, aes(x = as.Date(paste0("1999-",TED.df$month,"-01"),"%Y-%m-%d"), ,fill =topic)) + geom_bar(stat='count',position = position_dodge2(width = NULL, preserve = c("total", "single"),
  padding = 0.1, reverse = FALSE),size=2) + labs(title = "Counts of TOpic Videos",subtitle = "By Month and Year",x="Months",y="Count") + scale_x_date(date_labels = "%b") + facet_wrap(~year)
```





To help structure our consideration of predictors of popularity we implmented forward elimination process with logistc regression.
We began by spliting our data in 80% traing and 20% testing using stratified random sampeling. Our strata sizes selected proptionaly
based on the number of popular and unpopular avalible amongst the entire set. We sampled 400 Popular classifed videos and 1640 Unpopular classified videos
to be used in traing. The remaing would then be used a test set to evaluate the trained models

Logsitic Regression Analysis
```{r}
set.seed(123)
TED.df$index <- seq(1,2550,1)
notPopular <- filter(TED.df,isPop == 0)
notPopularSample <- notPopular %>% sample_n(1640)
Popular <- filter(TED.df, isPop== 1)
PopularSample <- Popular %>% sample_n(400)
train <- rbind(notPopularSample,PopularSample)
test <- TED.df[-train$index,]

accuracy <- function(model,train,test){
  predictions <- predict(model,type = "response")
  predTrain.df <- data.frame(p=predictions)
  predTrain.df <- mutate(predTrain.df,isPop = if_else(p >= .5,1,0))
  TrainAcc <- 1 - (sum(abs(train$isPop -predTrain.df$isPop)) / nrow(train))
  
  predictions <- predict(model,newdata = test,type = "response")
  predTest.df <- data.frame(p=predictions)
  predTest.df <- mutate(predTest.df,isPop = if_else(p >= .5,1,0))

  TestAcc <- 1 - (sum(abs(test$isPop - predTest.df$isPop)) / nrow(test))
  result <- c(TrainAcc,TestAcc)
  return(result)
}

modelResults <- data.frame(Variables=rep("a",8),AIC=rep(0,8),TrainACC = rep(0,8),TestACC=rep(0,8))
modelResults$Variables <- as.character(modelResults$Variables)
colnames(modelResults) <- c("Variables","AIC","TrainACC", "TestACC")


z <- 1

```

We begain by traing a binomial logsitic regression on each individual quantative and categorical varible. Each model was checked whether all coefficents p-values were less than 0.1 . If the model passed this test then the AIK score was repoted along with its training and test accuracy. After the first round of varible selection was completed the varible which h minimized the AIK was selected to be used in the two varible sequnce. We continued this process iterativley. At our 3rd varible selction p-value signifcance was then lowered to .05. We continoued itervaley selecting the varible that met this critera and minimized AIK up until a nine varible model. This is when no additonal varbile selctions had p-values that were significant.

```{r,include=F}
# 1 variable Models
varList <- c('languages','num_speaker','year','month','day','AvgRelViews','RelatedCount','Persuasive','Unconvincing','Longwinded','Informative','Beautiful','Confusing','Obnoxious','jawdropping','Ingenious','OK','Courageous', 'Inspiring','Funny','Fascinating','eventType','topic','titleLen','descriptionLen')
results <- matrix(,30,2)
index <- 1
n <- length(varList)
for(i in 1:n){
  fmla <- as.formula(paste0("isPop ~ ",varList[i]))
  model <- glm(fmla,data = train,family = binomial)
  sum <-summary(model)
  if (all(sum$coefficients[,4] < .1)){
    results[index,1] <- paste0(varList[i])
    results[index,2] <- as.numeric(sum$aic)
    index <- index + 1
  }
}
colnames(results)<- c("Variables","AIC")
results <- as.data.frame(results)
(results <-arrange(results,AIC))
results$Variables <- as.character(results$Variables)
r <- results[1,]
modelResults[z,1] <- r[1]
fmla <- fmla <- as.formula(paste0("isPop ~ ",results[1,1]))
m <- glm(fmla,train,family = binomial)
modelResults[z,2] <- m$aic
acc <- accuracy(m,train,test)
modelResults[z,3] <- acc[1]
modelResults[z,4] <- acc[2]
z <- z + 1
modelResults
```



```{r,include=F}
# 2 variable Models
varList <- c('num_speaker','year','month','day','AvgRelViews','RelatedCount','Persuasive','Unconvincing','Longwinded','Informative','Beautiful','Confusing','Obnoxious','jawdropping','Ingenious','OK','Courageous', 'Inspiring','Funny','Fascinating','eventType','topic','titleLen','descriptionLen')

results <- matrix(,30,2)
index <- 1
n <- length(varList)
for(i in 1:n){
  fmla <- as.formula(paste0("isPop ~ languages + ",varList[i]))
  model <- glm(fmla,data = train,family = binomial)
  sum <-summary(model)
  if (all(sum$coefficients[,4] < .1)){
    results[index,1] <- paste0("languages + ", varList[i])
    results[index,2] <- as.numeric(sum$aic)
    index <- index + 1
  }
}
colnames(results)<- c("Variables","AIC")
results <- as.data.frame(results)
(results <-arrange(results,AIC))
results$Variables <- as.character(results$Variables)
r <- results[1,]
modelResults[z,1] <- r[1]
fmla <- fmla <- as.formula(paste0("isPop ~ ",results[1,1]))
m <- glm(fmla,train,family = binomial)
modelResults[z,2] <- m$aic
acc <- accuracy(m,train,test)
modelResults[z,3] <- acc[1]
modelResults[z,4] <- acc[2]
z <- z + 1
modelResults
```


```{r,include=F}
# 3 variable Models
varList <- c('num_speaker','year','month','day','AvgRelViews','RelatedCount','OK','Unconvincing','Longwinded','Informative','Beautiful','Confusing','Obnoxious','jawdropping','Ingenious','Courageous', 'Inspiring','Funny','Fascinating','eventType','topic','titleLen','descriptionLen')

results <- matrix(,30,2)
index <- 1
n <- length(varList)
for(i in 1:n){
  fmla <- as.formula(paste0("isPop ~ languages + Persuasive + ",varList[i]))
  model <- glm(fmla, data = train,family = binomial)
  sum <-summary(model)
  if (all(sum$coefficients[,4] < .075)){
    results[index,1] <- paste0("languages + Persuasive + ", varList[i])
    results[index,2] <- as.numeric(sum$aic)
    index <- index + 1
  }
}
colnames(results)<- c("Variables","AIC")
results <- as.data.frame(results)
(results <-arrange(results,AIC))
results$Variables <- as.character(results$Variables)
r <- results[1,]
modelResults[z,1] <- r[1]
fmla <- fmla <- as.formula(paste0("isPop ~ ",results[1,1]))
m <- glm(fmla,train,family = binomial)
modelResults[z,2] <- m$aic
acc <- accuracy(m,train,test)
modelResults[z,3] <- acc[1]
modelResults[z,4] <- acc[2]
z <- z + 1
modelResults
```


```{r,include=F}
#4 Variable Models
varList <- c('num_speaker','year','month','day','AvgRelViews','RelatedCount','Unconvincing','Longwinded','Informative','Beautiful','Confusing','Obnoxious','jawdropping','Ingenious','Courageous', 'Inspiring','Funny','Fascinating','eventType','topic','titleLen','descriptionLen')


results <- matrix(,30,2)
index <- 1
n <- length(varList)
for(i in 1:n){
  fmla <- as.formula(paste0("isPop ~ languages + Persuasive + OK + ",varList[i]))
  model <- glm(fmla,data = train,family = binomial)
  sum <-summary(model)
  if (all(sum$coefficients[,4] < .075)){
    results[index,1] <- paste0("languages + Persuasive + OK + ", varList[i])
    results[index,2] <- as.numeric(sum$aic)
    index <- index + 1
  }
}
colnames(results)<- c("Variables","AIC")
results <- as.data.frame(results)
(results <-arrange(results,AIC))
results$Variables <- as.character(results$Variables)
r <- results[1,]
modelResults[z,1] <- r[1]
fmla <- fmla <- as.formula(paste0("isPop ~ ",results[1,1]))
m <- glm(fmla,train,family = binomial)
modelResults[z,2] <- m$aic
acc <- accuracy(m,train,test)
modelResults[z,3] <- acc[1]
modelResults[z,4] <- acc[2]
z <- z + 1
modelResults
```

```{r,include=F}
#5 Variable Models
varList <- c('num_speaker','year','month','day','AvgRelViews','RelatedCount','Longwinded','Informative','Beautiful','Confusing','Obnoxious','jawdropping','Ingenious','Courageous', 'Inspiring','Funny','Fascinating','eventType','topic','titleLen','descriptionLen')

results <- matrix(,30,2)
index <- 1
n <- length(varList)
for(i in 1:n){
  fmla <- as.formula(paste0("isPop ~ languages + Persuasive + OK + Unconvincing + ",varList[i]))
  model <- glm(fmla,data = train,family = binomial)
  sum <-summary(model)
  if (all(sum$coefficients[,4] < .075)){
    results[index,1] <- paste0("languages + Persuasive + OK + Unconvincing + ", varList[i])
    results[index,2] <- as.numeric(sum$aic)
    index <- index + 1
  }
}
colnames(results)<- c("Variables","AIC")
results <- as.data.frame(results)
(results <-arrange(results,AIC))
results$Variables <- as.character(results$Variables)
r <- results[1,]
modelResults[z,1] <- r[1]
fmla <- fmla <- as.formula(paste0("isPop ~ ",results[1,1]))
m <- glm(fmla,train,family = binomial)
modelResults[z,2] <- m$aic
acc <- accuracy(m,train,test)
modelResults[z,3] <- acc[1]
modelResults[z,4] <- acc[2]
z <- z + 1
modelResults

```


```{r,include=F}
#6 Variable Models
varList <- c('num_speaker','year','month','day','AvgRelViews','RelatedCount','Longwinded','Informative','Beautiful','Confusing','Obnoxious','jawdropping','Ingenious','Courageous', 'Inspiring','Funny','eventType','topic','titleLen','descriptionLen')

results <- matrix(,30,2)
index <- 1
n <- length(varList)
for(i in 1:n){
  fmla <- as.formula(paste0("isPop ~ languages + Persuasive + OK + Unconvincing + Fascinating + ",varList[i]))
  model <- glm(fmla,data = train,family = binomial)
  sum <-summary(model)
  if (all(sum$coefficients[,4] < .075)){
    results[index,1] <- paste0("languages + Persuasive + OK + Unconvincing + Fascinating + ", varList[i])
    results[index,2] <- as.numeric(sum$aic)
    index <- index + 1
  }
}
colnames(results)<- c("Variables","AIC")
results <- as.data.frame(results)
(results <-arrange(results,AIC))
results$Variables <- as.character(results$Variables)
r <- results[1,]
modelResults[z,1] <- r[1]
fmla <- fmla <- as.formula(paste0("isPop ~ ",results[1,1]))
m <- glm(fmla,train,family = binomial)
modelResults[z,2] <- m$aic
acc <- accuracy(m,train,test)
modelResults[z,3] <- acc[1]
modelResults[z,4] <- acc[2]
z <- z + 1
modelResults
```

```{r,include=F}
#7 Variable Models
varList <- c('num_speaker','year','month','day','AvgRelViews','RelatedCount','Longwinded','Informative','Beautiful','Confusing','Obnoxious','jawdropping','Ingenious', 'Inspiring','Funny','eventType','topic','titleLen','descriptionLen')

results <- matrix(,30,2)
index <- 1
n <- length(varList)
for(i in 1:n){
  fmla <- as.formula(paste0("isPop ~ languages + Persuasive + OK + Unconvincing + Fascinating + Courageous + ",varList[i]))
  model <- glm(fmla,data = train,family = binomial)
  sum <-summary(model)
  if (all(sum$coefficients[,4] < .075)){
    results[index,1] <- paste0("languages + Persuasive + OK + Unconvincing + Fascinating + Courageous + ", varList[i])
    results[index,2] <- as.numeric(sum$aic)
    index <- index + 1
  }
}
colnames(results)<- c("Variables","AIC")
results <- as.data.frame(results)
(results <-arrange(results,AIC))
results$Variables <- as.character(results$Variables)
r <- results[1,]
modelResults[z,1] <- r[1]
fmla <- fmla <- as.formula(paste0("isPop ~ ",results[1,1]))
m <- glm(fmla,train,family = binomial)
modelResults[z,2] <- m$aic
acc <- accuracy(m,train,test)
modelResults[z,3] <- acc[1]
modelResults[z,4] <- acc[2]
z <- z + 1
modelResults
```

```{r,incl}
#8 Variable Models
varList <- c('num_speaker','year','month','day','AvgRelViews','RelatedCount','Longwinded','Informative','Beautiful','Confusing','Obnoxious','Ingenious', 'Inspiring','Funny','eventType','topic','titleLen','descriptionLen')

results <- matrix(,30,2)
index <- 1
n <- length(varList)
for(i in 1:n){
  fmla <- as.formula(paste0("isPop ~ languages + Persuasive + OK + Unconvincing + Fascinating + Courageous + jawdropping + ",varList[i]))
  model <- glm(fmla,data = train,family = binomial)
  sum <-summary(model)
  if (all(sum$coefficients[,4] < .075)){
    results[index,1] <- paste0("languages + Persuasive + OK + Unconvincing + Fascinating + Courageous + jawdropping + ", varList[i])
    results[index,2] <- as.numeric(sum$aic)
    index <- index + 1
  }
}
colnames(results)<- c("Variables","AIC")
results <- as.data.frame(results)
(results <-arrange(results,AIC))
results$Variables <- as.character(results$Variables)
r <- results[1,]
modelResults[z,1] <- r[1]
fmla <- fmla <- as.formula(paste0("isPop ~ ",results[1,1]))
m <- glm(fmla,train,family = binomial)
modelResults[z,2] <- m$aic
acc <- accuracy(m,train,test)
modelResults[z,3] <- acc[1]
modelResults[z,4] <- acc[2]
z <- z + 1
modelResults
```

```{r}
#9 Variable Models
varList <- c('num_speaker','year','month','day','RelatedCount','Longwinded','Informative','Beautiful','Confusing','Obnoxious','Ingenious', 'Inspiring','Funny','eventType','topic','titleLen','descriptionLen')

results <- matrix(,30,2)
index <- 1
n <- length(varList)
for(i in 1:n){
  fmla <- as.formula(paste0("isPop ~ languages + Persuasive + OK + Unconvincing + Fascinating + Courageous + jawdropping + AvgRelViews + ",varList[i]))
  model <- glm(fmla,data = train,family = binomial)
  sum <-summary(model)
  if (all(sum$coefficients[,4] < .075)){
    results[index,1] <- paste0("languages + Persuasive + OK + Unconvincing + Fascinating + Courageous + jawdropping + AvgRelViews + ", varList[i])
    results[index,2] <- as.numeric(sum$aic)
    index <- index + 1
  }
}
colnames(results)<- c("Variables","AIC")
results <- as.data.frame(results)
(results <-arrange(results,AIC))

```



In comparing the top models for each 1 o 8 number of varibles we see that AIK continually decrease, training accuracy continously increase but at a diminishing rate, and
Trainging accuracy peaks at 6 varibles then begins to decrease with further addiotnal varibles.

```{r}
# Best Model
modelResults
modelResults$numVar <- c(1,2,3,4,5,6,7,8)
modelResults$TestACC <- as.numeric(modelResults$TestACC)
modelResults$TrainACC <- as.numeric(modelResults$TrainACC)
fmla <- as.formula(paste0("isPop ~ languages + Persuasive + OK + Unconvincing + Fascinating + Courageous"))
model <- glm(fmla,data = train,family = binomial)
summary(model)
# Evaluate Accuracy on the full data set
(accuracy(model,train,TED.df))

ggplot(modelResults,aes(x=Variables,y=AIC)) + geom_bar(stat='identity') + labs(title="AIC by number of varibles") + scale_x_discrete(labels = c('1','2','3','4','5','6','7','8'))
ggplot(modelResults,aes(x=numVar,y=TrainACC)) + geom_line() + labs(title="Training Accuracy by Number of varibles") 
ggplot(modelResults,aes(x=numVar,y=TestACC)) + geom_line() + labs(title="Test Accuracy by Number of varibles")


```
```{r }
TED.df$isPop <- as.character(TED.df$isPop)
m <-  melt(TED.df,id.vars='isPop',measure.vars=c('languages','titleLen'))
g <- ggplot(m,aes(x=isPop, y=value, fill=variable))
g + geom_bar(stat = "summary", fun.y = "mean",position = position_dodge(width = 0.9)) +  labs(title ="Predicting Varibles Averages" ,subtitle = "Languages and Title Length" , x="Popularity") + scale_x_discrete(labels = c("Not Popular","Popular"))

 
m <- melt(TED.df,id.vars='isPop',measure.vars=c('Persuasive', 'OK','Unconvincing','Fascinating','Courageous','jawdropping')) 

g <- ggplot(m,aes(x=isPop, y=value, fill=variable))
g + geom_bar(stat = "summary", fun.y = "mean",position = position_dodge(width = 0.9)) + labs(title = "Predicting Varibles Averagess",subtitle = "Ratio's of Ratings", x="Popularity") + scale_x_discrete(breaks = c("0","1"),labels = c("Not Popular","Popular"))
```
```{r}
TED.df <- mutate(TED.df,isPop= ifelse(isPop =="1","Popular","Not Popular"))
```



```{r}


m <-  melt(TED.df,id.vars=c('topic','isPop'),measure.vars=c('languages','titleLen'))
g <- ggplot(m,aes(x=topic, y=value, fill=variable))
g + geom_bar(stat = "summary", fun.y = "mean",position = position_dodge(width = 0.9)) +  labs(title ="Predicting Varibles Averages" ,subtitle = "Languages and Title Length by topic group" , x="Topic Group") + facet_wrap(~isPop)

 
m <-  melt(TED.df,id.vars=c('topic','isPop'),measure.vars=c('Persuasive', 'OK','Unconvincing','Fascinating','Courageous','jawdropping'))
g <- ggplot(m,aes(x=topic, y=value, fill=variable))
g + geom_bar(stat = "summary", fun.y = "mean",position = position_dodge(width = 0.9)) +  labs(title ="Predicting Varibles Averages" ,subtitle = "Ratings Ratios by Topic Group" , x="Topic Group") + facet_wrap(~isPop)
```


